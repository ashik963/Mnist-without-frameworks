{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from random import seed\n",
    "from random import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train TRAIN] [--trainlabel TRAINLABEL]\n",
      "                             [--test TEST] [--testlabel TESTLABEL] [--lr LR]\n",
      "                             [--epochs EPOCHS] [--n_x N_X] [--n_h1 N_H1]\n",
      "                             [--n_h2 N_H2] [--beta BETA]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "ipykernel_launcher.py: error: argument --train: invalid float value: 'train_image.csv'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3273: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# hyperparameters setting\n",
    "parser.add_argument('--train', type=float, default='train_image.csv', help='training image set')\n",
    "parser.add_argument('--trainlabel', type=float, default='train_label.csv', help='training label set')\n",
    "parser.add_argument('--test', type=float, default='test_image.csv', help='Testing image set')\n",
    "parser.add_argument('--testlabel', type=float, default='test_label.csv', help='Testing label set')\n",
    "parser.add_argument('--lr', type=float, default=0.1, help='learning rate')\n",
    "parser.add_argument('--epochs', type=int, default=50,\n",
    "                    help='number of epochs to train')\n",
    "parser.add_argument('--n_x', type=int, default=784, help='number of inputs')\n",
    "parser.add_argument('--n_h1', type=int, default=200,help='number of hidden units 1')\n",
    "parser.add_argument('--n_h2', type=int, default=100,help='number of hidden units 2')\n",
    "parser.add_argument('--beta', type=float, default=0.9,help='parameter for momentum')\n",
    "parser.add_argument('--batch_size', type=int,default=64, help='input batch size')\n",
    "\n",
    "# parse the arguments\n",
    "opt = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opt:\n",
    "    n_x=784\n",
    "    n_h1=200\n",
    "    n_h2=100\n",
    "    lr=0.1\n",
    "    beta=0.9\n",
    "    batch_size=64\n",
    "    epochs=150\n",
    "opt=Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('train_image.csv')\n",
    "train_label=pd.read_csv('train_label.csv')\n",
    "test_data=pd.read_csv('test_image.csv')\n",
    "test_label=pd.read_csv('test_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.float32(train_data.values)\n",
    "x_test=np.float32(test_data.values)\n",
    "y_train = np.int32(train_label.values).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stack together for next step\n",
    "# X = np.vstack((x_train, x_test))\n",
    "# y = np.vstack((y_train, y_test))\n",
    "\n",
    "\n",
    "# one-hot encoding\n",
    "digits = 10\n",
    "examples = y_train.shape[0]\n",
    "y = y_train.reshape(1, examples)\n",
    "Y_new = np.eye(digits)[y.astype('int32')]\n",
    "Y_new = Y_new.T.reshape(digits, examples)\n",
    "\n",
    "\n",
    "# number of training set\n",
    "m = 60000\n",
    "# m_test = X.shape[0] - m\n",
    "# X_train, X_test = X[:m].T, X[m:].T\n",
    "# Y_train, Y_test = Y_new[:, :m], Y_new[:, m:]\n",
    "X_train, X_test = x_train.T, x_test.T\n",
    "Y_train = Y_new\n",
    "\n",
    "\n",
    "\n",
    "# shuffle training set\n",
    "shuffle_index = np.random.permutation(m)\n",
    "X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 60000)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "params = {\"W1\": np.random.randn(opt.n_h1, opt.n_x) * np.sqrt(1. / opt.n_x),\n",
    "          \"b1\": np.zeros((opt.n_h1, 1)) * np.sqrt(1. / opt.n_x),\n",
    "          \"W2\": np.random.randn(opt.n_h2, opt.n_h1) * np.sqrt(1. / opt.n_h1),\n",
    "          \"b2\": np.zeros((opt.n_h2, 1)) * np.sqrt(1. / opt.n_h1),\n",
    "          \"W3\": np.random.randn(digits, opt.n_h2) * np.sqrt(1. / opt.n_h2),\n",
    "          \"b3\": np.zeros((digits, 1)) * np.sqrt(1. / opt.n_h2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    sigmoid activation function.\n",
    "\n",
    "    inputs: z\n",
    "    outputs: sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1. / (1. + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cross entropy loss\n",
    "def compute_loss(Y, Y_hat):\n",
    "    \"\"\"\n",
    "    compute loss function\n",
    "    \"\"\"\n",
    "    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "    m = Y.shape[1]\n",
    "    L = -(1./m) * L_sum\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X, params):\n",
    "    \"\"\"\n",
    "    feed forward network: 2 - layer neural net\n",
    "\n",
    "    inputs:\n",
    "        params: dictionay a dictionary contains all the weights and biases\n",
    "\n",
    "    return:\n",
    "        cache: dictionay a dictionary contains all the fully connected units and activations\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "\n",
    "    # Z1 = W1.dot(x) + b1\n",
    "    cache[\"Z1\"] = np.matmul(params[\"W1\"], X) + params[\"b1\"]\n",
    "\n",
    "    # A1 = sigmoid(Z1)\n",
    "    cache[\"A1\"] = sigmoid(cache[\"Z1\"])\n",
    "    \n",
    "    # Z2 = W2.dot(x) + b2\n",
    "    cache[\"Z2\"] = np.matmul(params[\"W2\"], cache[\"A1\"]) + params[\"b2\"]\n",
    "\n",
    "    # A2 = sigmoid(Z2)\n",
    "    cache[\"A2\"] = sigmoid(cache[\"Z2\"])\n",
    "\n",
    "    \n",
    "    # Z2 = W2.dot(A1) + b2\n",
    "    cache[\"Z3\"] = np.matmul(params[\"W3\"], cache[\"A2\"]) + params[\"b3\"]\n",
    "\n",
    "    # A2 = softmax(Z2)\n",
    "    cache[\"A3\"] = np.exp(cache[\"Z3\"]) / np.sum(np.exp(cache[\"Z3\"]), axis=0)\n",
    "\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(X, Y, params, cache, m_batch):\n",
    "    \"\"\"\n",
    "    back propagation\n",
    "\n",
    "    inputs:\n",
    "        params: dictionay a dictionary contains all the weights and biases\n",
    "        cache: dictionay a dictionary contains all the fully connected units and activations\n",
    "\n",
    "    return:\n",
    "        grads: dictionay a dictionary contains the gradients of corresponding weights and biases\n",
    "    \"\"\"\n",
    "    # error at last layer\n",
    "    dZ3 = cache[\"A3\"] - Y\n",
    "\n",
    "    # gradients at last layer (Py2 need 1. to transform to float)\n",
    "    dW3 = (1. / m_batch) * np.matmul(dZ3, cache[\"A2\"].T)\n",
    "    db3 = (1. / m_batch) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    # back propgate through first layer\n",
    "    dA2 = np.matmul(params[\"W3\"].T, dZ3)\n",
    "    dZ2 = dA2 * sigmoid(cache[\"Z2\"]) * (1 - sigmoid(cache[\"Z2\"]))\n",
    "\n",
    "    # gradients at first layer (Py2 need 1. to transform to float)\n",
    "    dW2 = (1. / m_batch) * np.matmul(dZ2,  cache[\"A1\"].T)\n",
    "    db2 = (1. / m_batch) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    # back propgate through first layer\n",
    "    dA1 = np.matmul(params[\"W2\"].T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid(cache[\"Z1\"]) * (1 - sigmoid(cache[\"Z1\"]))\n",
    "\n",
    "    # gradients at first layer (Py2 need 1. to transform to float)\n",
    "    dW1 = (1. / m_batch) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1. / m_batch) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2,\"dW3\": dW3, \"db3\": db3}\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 0.6080236558292749\n",
      "Epoch 2: training loss = 0.579512380446226\n",
      "Epoch 3: training loss = 0.5344610185841955\n",
      "Epoch 4: training loss = 0.5297215565374935\n",
      "Epoch 5: training loss = 0.5095608446004368\n",
      "Epoch 6: training loss = 0.4970887656995295\n",
      "Epoch 7: training loss = 0.47579035642164036\n",
      "Epoch 8: training loss = 0.4685964812034636\n",
      "Epoch 9: training loss = 0.4508739878446206\n",
      "Epoch 10: training loss = 0.44945806942841693\n",
      "Epoch 11: training loss = 0.46838651173745965\n",
      "Epoch 12: training loss = 0.4727691584249045\n",
      "Epoch 13: training loss = 0.44055456567993767\n",
      "Epoch 14: training loss = 0.45361085130020945\n",
      "Epoch 15: training loss = 0.44317764818497457\n",
      "Epoch 16: training loss = 0.44257930314892435\n",
      "Epoch 17: training loss = 0.43268026593071646\n",
      "Epoch 18: training loss = 0.4287212572318764\n",
      "Epoch 19: training loss = 0.45146563359915964\n",
      "Epoch 20: training loss = 0.4298746554495617\n",
      "Epoch 21: training loss = 0.4122834881006173\n",
      "Epoch 22: training loss = 0.41640621620831364\n",
      "Epoch 23: training loss = 0.4509474481840961\n",
      "Epoch 24: training loss = 0.41777183359681846\n",
      "Epoch 25: training loss = 0.4264103952617898\n",
      "Epoch 26: training loss = 0.45600127607730945\n",
      "Epoch 27: training loss = 0.42724114303626065\n",
      "Epoch 28: training loss = 0.44902666291943183\n",
      "Epoch 29: training loss = 0.43678425702408297\n",
      "Epoch 30: training loss = 0.4257150693016309\n",
      "Epoch 31: training loss = 0.43179619822642357\n",
      "Epoch 32: training loss = 0.43347424479205054\n",
      "Epoch 33: training loss = 0.4317426440628625\n",
      "Epoch 34: training loss = 0.4407224356207849\n",
      "Epoch 35: training loss = 0.4475894571435699\n",
      "Epoch 36: training loss = 0.4670986883419081\n",
      "Epoch 37: training loss = 0.43611121058947633\n",
      "Epoch 38: training loss = 0.46868232571115237\n",
      "Epoch 39: training loss = 0.42893698202965985\n",
      "Epoch 40: training loss = 0.41185615735270525\n",
      "Epoch 41: training loss = 0.4231930048409756\n",
      "Epoch 42: training loss = 0.4184061447325229\n",
      "Epoch 43: training loss = 0.4181743871956341\n",
      "Epoch 44: training loss = 0.4460780703511333\n",
      "Epoch 45: training loss = 0.426416533715839\n",
      "Epoch 46: training loss = 0.42846846927756055\n",
      "Epoch 47: training loss = 0.4179665519465749\n",
      "Epoch 48: training loss = 0.43360381263866515\n",
      "Epoch 49: training loss = 0.4173659945010679\n",
      "Epoch 50: training loss = 0.4064312441202914\n",
      "Epoch 51: training loss = 0.41922271799975735\n",
      "Epoch 52: training loss = 0.4387033228429607\n",
      "Epoch 53: training loss = 0.4125584870610495\n",
      "Epoch 54: training loss = 0.42155698511328327\n",
      "Epoch 55: training loss = 0.4631095407431524\n",
      "Epoch 56: training loss = 0.438767134598652\n",
      "Epoch 57: training loss = 0.41809462671301895\n",
      "Epoch 58: training loss = 0.4165888416568573\n",
      "Epoch 59: training loss = 0.44437244859143343\n",
      "Epoch 60: training loss = 0.4289943833953016\n",
      "Epoch 61: training loss = 0.41220136643881894\n",
      "Epoch 62: training loss = 0.406555178258032\n",
      "Epoch 63: training loss = 0.402473887746195\n",
      "Epoch 64: training loss = 0.3874629637045261\n",
      "Epoch 65: training loss = 0.40995026201177037\n",
      "Epoch 66: training loss = 0.38678965056326997\n",
      "Epoch 67: training loss = 0.4121123905901399\n",
      "Epoch 68: training loss = 0.41292339827669944\n",
      "Epoch 69: training loss = 0.4275644194104726\n",
      "Epoch 70: training loss = 0.4168673851411298\n",
      "Epoch 71: training loss = 0.41968146538853907\n",
      "Epoch 72: training loss = 0.39414402336619353\n",
      "Epoch 73: training loss = 0.40371749233624893\n",
      "Epoch 74: training loss = 0.40400426450173516\n",
      "Epoch 75: training loss = 0.4468496287102542\n",
      "Epoch 76: training loss = 0.46444934021518874\n",
      "Epoch 77: training loss = 0.4099427249802544\n",
      "Epoch 78: training loss = 0.40240447254286577\n",
      "Epoch 79: training loss = 0.40517054492519566\n",
      "Epoch 80: training loss = 0.377840407996567\n",
      "Epoch 81: training loss = 0.39297321007550656\n",
      "Epoch 82: training loss = 0.4342353400428404\n",
      "Epoch 83: training loss = 0.39979416339478147\n",
      "Epoch 84: training loss = 0.3841424670480402\n",
      "Epoch 85: training loss = 0.38928298425460295\n",
      "Epoch 86: training loss = 0.3951405081808423\n",
      "Epoch 87: training loss = 0.3897064611641465\n",
      "Epoch 88: training loss = 0.388996017770911\n",
      "Epoch 89: training loss = 0.42028709221498667\n",
      "Epoch 90: training loss = 0.38463821227706824\n",
      "Epoch 91: training loss = 0.3932029410036494\n",
      "Epoch 92: training loss = 0.4244117178450823\n",
      "Epoch 93: training loss = 0.3991249295619614\n",
      "Epoch 94: training loss = 0.3986862800523337\n",
      "Epoch 95: training loss = 0.37976651786535826\n",
      "Epoch 96: training loss = 0.39550873504680545\n",
      "Epoch 97: training loss = 0.3869310066231156\n",
      "Epoch 98: training loss = 0.3903088624503744\n",
      "Epoch 99: training loss = 0.3758453293808398\n",
      "Epoch 100: training loss = 0.378877298948125\n",
      "Epoch 101: training loss = 0.39757432461312037\n",
      "Epoch 102: training loss = 0.40155649847465397\n",
      "Epoch 103: training loss = 0.386629231571979\n",
      "Epoch 104: training loss = 0.3937792153023613\n",
      "Epoch 105: training loss = 0.3977972906202084\n",
      "Epoch 106: training loss = 0.416126356372809\n",
      "Epoch 107: training loss = 0.44971239588187406\n",
      "Epoch 108: training loss = 0.449068813541444\n",
      "Epoch 109: training loss = 0.4164294682352663\n",
      "Epoch 110: training loss = 0.43803284690068733\n",
      "Epoch 111: training loss = 0.4223408455339385\n",
      "Epoch 112: training loss = 0.41269122328979785\n",
      "Epoch 113: training loss = 0.42996061919815975\n",
      "Epoch 114: training loss = 0.41229818010616354\n",
      "Epoch 115: training loss = 0.3965928155926556\n",
      "Epoch 116: training loss = 0.4086095160657907\n",
      "Epoch 117: training loss = 0.42330253858977174\n",
      "Epoch 118: training loss = 0.4096943230640796\n",
      "Epoch 119: training loss = 0.4199837595369948\n",
      "Epoch 120: training loss = 0.41267673893439366\n",
      "Epoch 121: training loss = 0.38940246890912583\n",
      "Epoch 122: training loss = 0.42183084686653116\n",
      "Epoch 123: training loss = 0.37651064159333353\n",
      "Epoch 124: training loss = 0.38223878885338936\n",
      "Epoch 125: training loss = 0.4058562193212177\n",
      "Epoch 126: training loss = 0.40237126470561035\n",
      "Epoch 127: training loss = 0.39489885390432594\n",
      "Epoch 128: training loss = 0.39035214635547116\n",
      "Epoch 129: training loss = 0.4209659011836473\n",
      "Epoch 130: training loss = 0.39686830553259966\n",
      "Epoch 131: training loss = 0.4050610337028419\n",
      "Epoch 132: training loss = 0.4016530819281375\n",
      "Epoch 133: training loss = 0.41791102288851617\n",
      "Epoch 134: training loss = 0.40079026038577953\n",
      "Epoch 135: training loss = 0.42334662758111097\n",
      "Epoch 136: training loss = 0.40164390386370175\n",
      "Epoch 137: training loss = 0.41456269604829077\n",
      "Epoch 138: training loss = 0.414089543809715\n",
      "Epoch 139: training loss = 0.3853438091905743\n",
      "Epoch 140: training loss = 0.3966411265049515\n",
      "Epoch 141: training loss = 0.4108978258110081\n",
      "Epoch 142: training loss = 0.40121341772040664\n",
      "Epoch 143: training loss = 0.4153392843034828\n",
      "Epoch 144: training loss = 0.38986670601088597\n",
      "Epoch 145: training loss = 0.394569220043376\n",
      "Epoch 146: training loss = 0.4045555492910922\n",
      "Epoch 147: training loss = 0.3877103906740808\n",
      "Epoch 148: training loss = 0.40500192439207494\n",
      "Epoch 149: training loss = 0.42397381555974545\n",
      "Epoch 150: training loss = 0.3914791516761757\n"
     ]
    }
   ],
   "source": [
    "for i in range(opt.epochs):\n",
    "    seed(1234)\n",
    "    # shuffle training set\n",
    "    permutation = np.random.permutation(X_train.shape[1])\n",
    "    X_train_shuffled = X_train[:, permutation]\n",
    "    Y_train_shuffled = Y_train[:, permutation]\n",
    "\n",
    "    for j in range(opt.batch_size):\n",
    "\n",
    "        # get mini-batch\n",
    "        begin = j * opt.batch_size\n",
    "        end = min(begin + opt.batch_size, X_train.shape[1] - 1)\n",
    "        X = X_train_shuffled[:, begin:end]\n",
    "        Y = Y_train_shuffled[:, begin:end]\n",
    "        m_batch = end - begin\n",
    "\n",
    "        # forward and backward\n",
    "        cache = feed_forward(X, params)\n",
    "        grads = back_propagate(X, Y, params, cache, m_batch)\n",
    "\n",
    "        # with momentum (optional)\n",
    "        dW1 = (opt.beta * grads[\"dW1\"] + (1. - opt.beta) * grads[\"dW1\"])\n",
    "        db1 = (opt.beta * grads[\"db1\"] + (1. - opt.beta) * grads[\"db1\"])\n",
    "        dW2 = (opt.beta * grads[\"dW2\"] + (1. - opt.beta) * grads[\"dW2\"])\n",
    "        db2 = (opt.beta * grads[\"db2\"] + (1. - opt.beta) * grads[\"db2\"])\n",
    "        dW3 = (opt.beta * grads[\"dW3\"] + (1. - opt.beta) * grads[\"dW3\"])\n",
    "        db3 = (opt.beta * grads[\"db3\"] + (1. - opt.beta) * grads[\"db3\"])\n",
    "\n",
    "        # gradient descent\n",
    "        params[\"W1\"] = params[\"W1\"] - opt.lr * dW1\n",
    "        params[\"b1\"] = params[\"b1\"] - opt.lr * db1\n",
    "        params[\"W2\"] = params[\"W2\"] - opt.lr * dW2\n",
    "        params[\"b2\"] = params[\"b2\"] - opt.lr * db2\n",
    "        params[\"W3\"] = params[\"W3\"] - opt.lr * dW3\n",
    "        params[\"b3\"] = params[\"b3\"] - opt.lr * db3\n",
    "\n",
    "    # forward pass on training set\n",
    "    cache = feed_forward(X_train, params)\n",
    "    train_loss = compute_loss(Y_train, cache[\"A3\"])\n",
    "\n",
    "#     # forward pass on test set\n",
    "#     cache = feed_forward(X_test, params)\n",
    "#     test_loss = compute_loss(Y_test, cache[\"A3\"])\n",
    "    print(\"Epoch {}: training loss = {}\".format(\n",
    "        i + 1, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = feed_forward(X_test, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8857"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,np.argmax(cache[\"A3\"],axis=0).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.argmax(cache[\"A3\"],axis=0).reshape(-1,1)).to_csv('test_predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
